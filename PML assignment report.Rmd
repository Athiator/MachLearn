---
title: "Practical machine learning project"
author: "Andrew Reid"
date: "21 September 2014"
output: html_document
---

## Summary

This is a report responding to the course project for practical machine learning.

The goal is to use accelerometer data from 6 participants to determine whether barbell lifts they were performing were correct or not. The data source for this project is http://groupware.les.inf.puc-rio.br/har

## Analysis

First step is reading in the data and relevant libraries, after which the training data will be split into a subset for training and another subset to treat as a validation set for the model. Read.table is used instead of read.csv to ensure blank entries in many columns register as NA, rather than as a factor column with many entries at the 'blank' level.

```{r}
setwd("~/Rworking/MachLearn")
training <- read.table("./pml-training.csv", header = TRUE, sep = ",", fill = TRUE, na.strings = c("NA", ""))
library(caret)
library(kernlab)

##training <- fullTraining[,-(1:7)]



set.seed(38658)
inSubTraining<-createDataPartition(y=training$classe,p=0.75,list=FALSE)
subTraining <-training[inSubTraining,]
validation <- training[-inSubTraining,]

```

The goal is to produce a model from the subset of the training data to predict the "classe"" variable, after which the validation subset will be used to test the model, and the model will then be applied to the test data. 

I'm going to strip out a number of entries that aren't interesting to predict on, the name of the participant, the time of the event, and the window. This info is all in the first 7 columns of the table. I'll also clear out any column with an NA count over 10,000, about 2/3rds of the observations. Trying to run an imputation on columns with so little data would be more likely to throw the model astray. 

```{r}

cutColumn <- 1:160
cutColumn[1:7] <-0
i<-0

for(i in 8:160) {
  if (sum(is.na(subTraining[,i])) > 10000) {
    cutColumn[i] <- 0
  }
}


naRmTraining <- subTraining[,cutColumn]

```

The data has now been reduced to only 53 variables, and now we can have a look at generating a model. 

First pass at a model using linear discriminate analysis gives a relatively straightforward model.


```{r}
ldaFit <- train(classe ~ . , data = naRmTraining, method = "lda")

```

A quick check against the validation set will show the model to not be especially accurate, but it is doing much better than chance. The validation set obviously needs to have the same columns removed as for the training set first.

```{r}
cutValidation <- validation[,cutColumn]
ldaPredictions <- predict(ldaFit,cutValidation)

confusionMatrix(cutValidation$classe, ldaPredictions)

```

The low accuracy isn't really a surprise given that it was just a linear fit. Random forests would provide a much more accurate fit. However, an attempt to simply model with method = 'rf' didn't complete in a reasonable time. In order to get the model to compute we'll cut down on some of the total variance using the principle component analysis preprocessing in caret. Choosing a threshold of 80% should retain good predictive value while cutting down on the number of variables substantially. This allows the rf fit to complete in an acceptable period of time.


```{r}
pcaPreProc<-preProcess(naRmTraining[,-53],method="pca",thresh = 0.80 )
trainPreProc <-predict(pcaPreProc, naRmTraining[,-53])

rfFit<- train(naRmTraining$classe ~ . , method= "rf", data = trainPreProc)

```

Checking the rf fit against the validation set requires applying the PCA breakdown to the validation data, but after that the confusion matrix is easy to observe.

```{r}
pcaValidation <- predict(pcaPreProc, cutValidation[,-53])
rfPredictions <- predict(rfFit,pcaValidation)
confusionMatrix(cutValidation$classe, rfPredictions)

```

The 95% confidence interval for accuracy on the validation set with the rf model is 96-97%, which is a lot better than the linear discriminate model managed. It's also good enough to run with for predicting the test data. Given the validation set was not involved in the training of the model, that should represent a respectable estimate of the out of sample error.

This model has been applied to the test data for the assignment using the following code, with the results submitted via the automated assessment.

```{r}
 testing <- read.table("./pml-testing.csv", header = TRUE, sep = ",", fill = TRUE, na.strings = c("NA", ""))
cutTesting <- testing[,cutColumn]
pcaTesting <- predict(pcaPreProc, cutTesting[,-53])
rfTestPredictions <- predict(rfFit,pcaTesting)
```
